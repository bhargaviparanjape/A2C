from __future__ import print_function

import sys
import argparse
import numpy as np
np.random.seed(0)
import keras
import gym
import pickle as pkl

from keras.optimizers import Adam


class Imitation():
    def __init__(self, env, args):
        # Loading the expert model.
        self.args = args
        self.env = env
        self.nS = self.env.observation_space.shape[0]
        self.nA = self.env.action_space.n

        with open(self.args.model_config_path, 'r') as f:
            self.expert = keras.models.model_from_json(f.read())
        self.expert.load_weights(self.args.expert_weights_path)
        
        # Initializing the cloned model (to be trained).
        with open(self.args.model_config_path, 'r') as f:
            self.model = keras.models.model_from_json(f.read())
        
        if self.args.resume or self.args.testonly:
        	self.model.load_weights(self.args.trained_model_path)
        	self.epochs_done, self.acchist, self.losshist = pkl.load(open(self.args.meta_path,'rb'))
        else:
        	self.epochs_done, self.acchist, self.losshist = 0, [], []

        self.optimizer = Adam(lr = self.args.lr)
        self.model.compile(loss='categorical_crossentropy',
              			   optimizer=self.optimizer, metrics=['accuracy'])

    def run_expert(self):
        # Generates an episode by running the expert policy on the given env.
        return Imitation.generate_episode(self.expert, self.env)

    def run_model(self):
        # Generates an episode by running the cloned policy on the given env.
        return Imitation.generate_episode(self.model, self.env)

    @staticmethod
    def generate_episode(model, env):
        # Generates an episode by running the given model on the given env.
        # Returns:
        # - a list of states, indexed by time step
        # - a list of actions, indexed by time step
        # - a list of rewards, indexed by time step

        states = []
        actions = []
        rewards = []
        
        current_state = env.reset()
        is_terminal  = False
        while not is_terminal:
            action_distribution = model.predict(np.expand_dims(current_state, 0))
            action = np.random.choice(env.action_space.n, 1, 
                                      p=action_distribution.squeeze(0))[0]
            next_state, reward, is_terminal, _ = env.step(action)
            states.append(current_state)
            actions.append(action)
            rewards.append(reward)
            current_state = next_state
            
        return states, actions, rewards
    
    def train(self):
        # Trains the model on training data generated by the expert policy.
        # Args:
        # - env: The environment to run the expert policy on. 
        # - num_episodes: # episodes to be generated by the expert.
        # - num_epochs: # epochs to train on the data generated by the expert.
        # - render: Whether to render the environment.
        # Returns the final loss and accuracy.
        if self.args.resume:
        	trainX, trainY = pkl.load(open(self.args.data_path,'rb'))

       	else:
	        trainX = []
	        trainy = []
	       	for i in range(self.args.num_episodes):
	       		states, actions, _ = self.run_expert()
	       		trainX.extend(states)
	       		trainy.extend(actions)

	       	trainX = np.array(trainX)
	       	trainY = np.zeros((len(trainy), self.nA))
	       	trainY[np.arange(len(trainy)), np.array(trainy)] = 1

	       	if not self.args.trial:
	       		pkl.dump((trainX, trainY), open(self.args.data_path,'wb'))

       	print ('*'*80)
       	print ('Data Collection Complete')
       	print ('*'*80)

       	hist = self.model.fit(trainX, trainY, batch_size = self.args.batch_size,
       			  verbose = self.args.verbose, epochs = (self.args.num_epochs-self.epochs_done))

       	self.acchist.extend(hist.history['acc'])
       	self.losshist.extend(hist.history['loss'])

       	if not self.args.trial:
	       	self.model.save_weights(self.args.trained_model_path)
	       	pkl.dump((self.args.num_epochs, self.acchist, self.losshist), open(self.args.meta_path,'wb'))


    def test(self, model):
        
        all_rewards = []
        for i in range(self.args.num_test_episodes):
            _ ,_ , rewards = Imitation.generate_episode(model, self.env)
            episode_reward = np.sum(rewards)
            all_rewards.append(episode_reward)
        
        average_reward = np.mean(all_rewards)
        std_reward = np.std(all_rewards)
        return average_reward, std_reward


def parse_arguments():
    # Command-line flags are defined here.
    parser = argparse.ArgumentParser()
    parser.add_argument('--model-config-path', dest='model_config_path', type=str,
                        default='LunarLander-v2-config.json',
                        help="Path to the model config file.")
    parser.add_argument('--expert-weights-path', dest='expert_weights_path', type=str, 
    					default='LunarLander-v2-weights.h5',
                        help="Path to the expert weights file.")
    parser.add_argument('--trained_model_path', dest='trained_model_path',type=str, 
                        default='imitation/imitation_model.h5',
                        help="Path to the model.")
    parser.add_argument('--meta_path', dest='meta_path',type=str, 
                        default='imitation/imitation_meta.p',
                        help="Path to meta information for model.")
    parser.add_argument('--data_path', dest='data_path',type=str, 
                        default='imitation/imitation_data.p',
                        help="Path to data")

    # https://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse
    parser_group = parser.add_mutually_exclusive_group(required=False)
    parser_group.add_argument('--render', dest='render',
                              action='store_true',
                              help="Whether to render the environment.")
    parser_group.add_argument('--no-render', dest='render',
                              action='store_false',
                              help="Whether to render the environment.")
    parser.set_defaults(render=False)

    parser.add_argument('--num-episodes', dest='num_episodes', type=int, 
                        default=100, 
                        help="Number of episodes to generate from expert.")
    parser.add_argument('--num-test-episodes', dest='num_test_episodes', type=int, 
                        default=50, 
                        help="Number of episodes to generate from expert.")
    parser.add_argument('--num-epochs', dest='num_epochs', type=int, 
                        default=50, 
                        help="Number of epochs to trained cloned model")
    parser.add_argument('--lr', dest='lr', type=float,
                        default=1e-4, help="Learning rate of model")

    parser.add_argument('--seed', type=int, dest='seed', default=0)
    parser.add_argument('--batch_size', type=int, dest='batch_size', default=32)

    parser.add_argument('--verbose', dest='verbose', action='store_true',
                         help="Whether to print loss after every episode.")
    parser.add_argument('--resume', dest='resume', type=int, default=0, 
                        help="Whether to resume the training from last checkpoint")

    parser.add_argument('--trial', dest='trial', action='store_true',
                         help="If it is just a trial")
    parser.add_argument('--testonly', dest='testonly', action='store_true',
                         help="If it is just a test")

    args = parser.parse_args()
    args.trained_model_path = '.'.join([args.trained_model_path.split('.')[0] 
    									+ str(args.num_episodes), 
    									args.trained_model_path.split('.')[1]])
    args.meta_path = '.'.join([args.meta_path.split('.')[0] + str(args.num_episodes), 
                                args.meta_path.split('.')[1]])
    args.data_path = '.'.join([args.data_path.split('.')[0] + str(args.num_episodes), 
                                args.data_path.split('.')[1]])

    return args


def main(args):
    # Parse command-line arguments.
    args = parse_arguments()
    
    # Create the environment.
    env = gym.make('LunarLander-v2')
    env.seed(args.seed)
    
    imitation = Imitation(env, args)

    if (not args.testonly) and (args.num_epochs - imitation.epochs_done) > 0:
    	imitation.train()

    print('*'*80)
    print('Accuracy of Trained Cloned Model: %f' %(imitation.acchist[-1]))
    print('*'*80)

    if args.testonly:

	    imitation.env.seed(args.seed)

	    average_reward_e, std_reward_e = imitation.test(imitation.expert)
	    print('*'*80)
	    print("Expert reward {0} +/- {1}".format(average_reward_e, std_reward_e))
	    print('*'*80)
	    
	    average_reward_c, std_reward_c = imitation.test(imitation.model)
	    print('*'*80)
	    print("Cloned using {0} episodes, Reward after {1} epochs: {2} +/- {3}"
	    		.format(args.num_episodes, args.num_epochs, average_reward_c, std_reward_c))
	    print('*'*80)

if __name__ == '__main__':
  main(sys.argv)
